{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fMRI analysis and visualization - subject averaged data \n",
    "\n",
    "#### In this notebook we will do some visualization and analysis of fMRI data which has already been averaged across participants.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from matplotlib import pyplot as plt \n",
    "from hdf5storage import loadmat, savemat \n",
    "from nilearn import plotting,datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you want to keep the code in the github repository\n",
    "datapath = '/home/ramesh/Teaching/data_archive/hcp_task/'\n",
    "#use this if you placed this program in the same location as the data. \n",
    "#datapath = './'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = np.load(datapath+'regions.npy') # this is the file \n",
    "roi_names = regions[:,0] # these are the names of each of 360 roi from the parcellation.\n",
    "network_names = regions[:,1] # these are the networks each roi \"belongs\" to\n",
    "networks = np.unique(regions[:,1]) # these are the unique network names "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the hcp atlas file.  This file provides a mapping between the 360 roi and the fsaverage (FreeSurfer Average) brain.  fsaverage is the average of 40 brains of healthy individuals.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlas = dict(np.load(datapath+'hcp_atlas.npz'))\n",
    "fsaverage = datasets.fetch_surf_fsaverage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loadmat(datapath+'SOCIAL_fmri_subjectaverage.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_index = data['condition_index']\n",
    "conditions = data['conditions']\n",
    "fmri = data['fmri']\n",
    "nconditions = data['nconditions']\n",
    "nregions = data['nregions']\n",
    "nsubjects = data['nsubjects']\n",
    "subject = data['subject']\n",
    "task = data['task']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## README \n",
    "\n",
    "#### condition_index - index for each data sample, indicating the experimental condition \n",
    "#### conditions - conditions in the experiment \n",
    "#### fmri - fmri data averaged over participants, nregions x (nsubjects x nconditions)\n",
    "#### nconditions - number of conditions\n",
    "#### nregions - number of regions (always 360)\n",
    "#### nsubjects - number of subjects (always 100)\n",
    "#### subject - indexes which subject each average comes from. \n",
    "#### task - which task the data comes from.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conditions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the SOCIAL task \n",
    "* #### 'mental' - appearance of social interaction \n",
    "* #### 'rnd' - appears to be random \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.  Standardize the data using z-score, and compute the different between conditions. Make a bar graph of the difference versus roi. Print out the roi_names and network_names of the 5 regions where activity in `mental` is most higher than `rnd`.  Print out the roi_names and network_names of the 5 regions where `rnd` is most higher than `mental`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "z = zscore(fmri)\n",
    "#z = zscore(fmri,axis = 0)\n",
    "diff = np.mean((z[:,condition_index == 0]-z[:,condition_index == 1]),axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bar plot of differences to orient to effect size.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,8))\n",
    "plt.bar(np.arange(nregions),diff)\n",
    "plt.xlabel('ROI')\n",
    "plt.ylabel('Mental-Random (sd units)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I am going to sort the effect size (absolute value of differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difforder = np.argsort(np.abs(diff))  # python sorting is always ascending.  so the first 5 here are big negative numbers. \n",
    "difforder = np.flipud(difforder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now print out the 10 regions with the highest magnitude differences.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('10 regions with strongest differences')\n",
    "for j in range(10):\n",
    "    print('ROI: ', roi_names[difforder[j]], 'Network: ', network_names[difforder[j]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform a Linear Discriminant Analysis to find the discriminant model to moximize differences between conditions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_T = np.transpose(z) # sci kit learn likes variables in columns \n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "fmri_lda = LDA(n_components=1) # the highest number of components is the number of classes - 1\n",
    "fmri_lda.fit(z_T, condition_index) # this fits a model\n",
    "data_lda = fmri_lda.transform(z_T) # this transforms the data into the component space\n",
    "data_lda = np.squeeze(data_lda) # remove unneeded dimensionality in 2 class models \n",
    "coef_lda = fmri_lda.coef_ # get the coefficient equation. \n",
    "coef_lda = np.squeeze(coef_lda) # squeeze unneeded dimensions.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize transformed data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist((data_lda[condition_index ==0]),bins=np.arange(-10,10,0.5),color = 'r')\n",
    "plt.hist((data_lda[condition_index ==1]),bins=np.arange(-10,10,0.5), color = 'b')\n",
    "plt.xlabel('LDA 1')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the transformation? \n",
    "#### The coef vector contains the **weight** on each ROI to maximally discriminate the 2 classes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surf_label = coef_lda[atlas['labels_R']] ## This maps the 360 values onto every voxel on the ROI in the brain.  \n",
    "plotting.view_surf(fsaverage['infl_right'],surf_label,symmetric_cmap = True, title = 'cmax_mental',black_bg = True, vmax = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surf_label = coef_lda[atlas['labels_L']] ## This maps the 360 values onto every voxel on the ROI in the brain.  \n",
    "plotting.view_surf(fsaverage['infl_left'],surf_label,symmetric_cmap = True, title = 'cmax_mental',black_bg = True, vmax = 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The weights can be used to identify which were the features of the original data that contrubuted to classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_r = np.argsort(np.abs(coef_lda))\n",
    "coef_r = np.flipud(coef_r)\n",
    "print('Regions contributing to classifier')\n",
    "for j in range(10):\n",
    "    print(roi_names[coef_r[j]],' Weight: ', coef_lda[coef_r[j]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We learnt last time that the correct way to do a classification analysis is to make use of cross-validation to confirm the predictive value of the model.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import KFold \n",
    "kf = KFold(n_splits = 5,shuffle = True)  # Here I told it to shuffle the data, and to make 5 splits of the data. \n",
    "errors = 0 \n",
    "predictions = np.zeros(np.shape(z_T)[0]) # here I save the prediction of each sample, when it was tested. \n",
    "probability = np.zeros((len(predictions),nconditions))\n",
    "for train_index, test_index in kf.split(z_T):\n",
    "#    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    data_train = z_T[train_index]\n",
    "    data_test = z_T[test_index]\n",
    "    Label_train= condition_index[train_index] \n",
    "    Label_test = condition_index[test_index]\n",
    "    lda = LDA(n_components=1)\n",
    "    data_model = lda.fit(data_train, Label_train)\n",
    "    predict = lda.predict(data_test)\n",
    "    test = Label_test == predict\n",
    "    errors = errors + sum(~test)\n",
    "    predictions[test_index] = predict\n",
    "    probability[test_index] = lda.predict_proba(data_test)\n",
    "errorrate = errors/len(condition_index)\n",
    "print(errorrate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can compute a confusion matrix to look at the pattern of errors.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = np.zeros((nconditions,nconditions))\n",
    "for j in range(nconditions):\n",
    "    values, counts = np.unique(predictions[condition_index == j],return_counts = True)\n",
    "    confusion_matrix[j,values.astype(int)] = counts\n",
    "print(confusion_matrix)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('fmri')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bb12961ae35e1f2bf7290dfae436ae47266196d96aa8209b8ac7c43d65127139"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
